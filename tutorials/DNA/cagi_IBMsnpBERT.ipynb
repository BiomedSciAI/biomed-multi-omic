{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the file now get the embeddings.... \n",
    "This is bascially now reproducing the results for zero-shot pre-trained model...\n",
    "We first generate the embeddings from our pre-trained model and then get a cosine distance...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_disabled_torch_function_impl' from 'torch._C' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbmfm_targets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbmfm_targets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dnaseq\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbmfm_targets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m task_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/bmfm_targets/config/__init__.py:12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mConfig objects for configuring models and runs.\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[33;03mThese are the underlying objects that are referenced in the config yamls.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     FieldInfo,\n\u001b[32m      8\u001b[39m     LabelColumnInfo,\n\u001b[32m      9\u001b[39m     TokenizerConfig,\n\u001b[32m     10\u001b[39m     PreTrainedEmbeddingConfig,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     TrainerConfig,\n\u001b[32m     14\u001b[39m     TrainingTaskConfig,\n\u001b[32m     15\u001b[39m     PredictTaskConfig,\n\u001b[32m     16\u001b[39m     TestTaskConfig,\n\u001b[32m     17\u001b[39m     InterpretTaskConfig,\n\u001b[32m     18\u001b[39m     LoraConfigWrapper,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     SCBertConfig,\n\u001b[32m     22\u001b[39m     SCPerformerConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     SCModernBertConfig,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SCBertMainConfig, SCBertMainHydraConfigSchema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/bmfm_targets/config/training_config.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Literal\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     LoraConfig \u001b[38;5;28;01mas\u001b[39;00m PeftLoraConfig,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_callbacks\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/pytorch_lightning/__init__.py:25\u001b[39m\n\u001b[32m     22\u001b[39m     _logger.addHandler(logging.StreamHandler())\n\u001b[32m     23\u001b[39m     _logger.propagate = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_fabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_fabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/lightning_fabric/__init__.py:35\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m\"\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     32\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mUSE_LIBUV\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_fabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfabric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_fabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_fabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/lightning_fabric/fabric.py:30\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     Any,\n\u001b[32m     22\u001b[39m     Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     overload,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply_func\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_to_collection\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_overridden\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/torch/nn/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/proj/bmfm/users/sanjoy/miniforge3/envs/bmfm-targets/lib/python3.11/site-packages/torch/nn/parameter.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ParameterMeta\u001b[39;00m(torch._C._TensorMeta):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_disabled_torch_function_impl' from 'torch._C' (unknown location)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "\n",
    "#import demo_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from bmfm_targets import config\n",
    "from bmfm_targets.datasets import dnaseq\n",
    "from bmfm_targets.tasks import task_utils\n",
    "from bmfm_targets.tokenization import load_tokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/bmfm-targets1/users/sanjoy/bmfm-targets/bmfm_targets/datasets/dnaseq/preprocess_dataset\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m/dccstor/bmfm-targets/users/hongyang/training_runs/scbert_ref_2kb_change0.3/epoch=17-step=264312-val_loss=5.42.ckpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m#/dccstor/bmfm-targets/users/hongyang/training_runs/scbert_snp_2kb/epoch=19-step=293680-val_loss=5.32.ckpt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#checkpoint_path = \"/proj/bmfm/users/hongyang/training_runs/ref_rc_1kb_10kb_10x_modernbert_v3/backup_ckpt/epoch=18-step=138282-val_loss=4.40.ckpt\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ckpt_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:1360\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1358\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1359\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1368\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:1848\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   1847\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1851\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:1812\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:1784\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1779\u001b[39m         storage.byteswap(dtype)\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1783\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     wrap_storage=\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1785\u001b[39m     dtype=dtype,\n\u001b[32m   1786\u001b[39m     _internal=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1787\u001b[39m )\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typed_storage._data_ptr() != \u001b[32m0\u001b[39m:\n\u001b[32m   1790\u001b[39m     loaded_storages[key] = typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:601\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:539\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    537\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/bmfm-targets/users/sanjoy/targets_env/anaconda/envs/bmfm_targets/lib/python3.11/site-packages/torch/serialization.py:508\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    506\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    516\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "# The pre-trained model...\n",
    "checkpoint_path = \"/dccstor/bmfm-targets/users/hongyang/training_runs/scbert_ref_2kb_change0.3/epoch=17-step=264312-val_loss=5.42.ckpt\" #/dccstor/bmfm-targets/users/hongyang/training_runs/scbert_snp_2kb/epoch=19-step=293680-val_loss=5.32.ckpt\n",
    "#checkpoint_path = \"/proj/bmfm/users/hongyang/training_runs/ref_rc_1kb_10kb_10x_modernbert_v3/backup_ckpt/epoch=18-step=138282-val_loss=4.40.ckpt\"\n",
    "\n",
    "ckpt_dict = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataloader for this SNV_MPRA dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"model_config\" in ckpt_dict[\"hyper_parameters\"]:\n",
    "#     fields = [i for i in ckpt_dict[\"hyper_parameters\"][\"model_config\"].fields if i.is_input]\n",
    "# else:\n",
    "#     fields = config.main_config.default_fields()\n",
    "#     tokenizer = load_tokenizer(Path(checkpoint_path).parent)\n",
    "#     for f in fields:\n",
    "#         f.update_vocab_size(tokenizer)\n",
    "#     processed_name = \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FieldInfo(field_name='dna_chunks', vocab_size=4096, pretrained_embedding=None, vocab_update_strategy='static', is_masked=True, is_input=True, masked_output_modes=['token_scores'], tokenization_strategy='tokenize', num_special_tokens=0, continuous_value_encoder_kwargs=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_data_source': '/dccstor/bmfm-targets1/data/omics/genome/snv_mpra_cagi_regression/data_imputed.csv', 'dataset_name': 'cagi_4096'}\n"
     ]
    }
   ],
   "source": [
    "#from bmfm_targets.datasets import dnaseq\n",
    "\n",
    "dataset_kwargs={\"processed_data_source\":\"/dccstor/bmfm-targets1/data/omics/genome/snv_mpra_cagi_regression/data_imputed.csv\", \n",
    "                    \"dataset_name\": \"cagi_4096\", \n",
    "                    #\"processed_file_name\": \"processed_imputed_snp2vec\",\n",
    "                    #\"label_dict_path\" : \"/dccstor/bmfm-targets/data/omics/genome/MPRA/LLM_eval/data/CAGI/4096/cagi_4096_all_labels.json\"\n",
    "                    }\n",
    "print(dataset_kwargs)\n",
    "\n",
    "# predict_dataset = dnaseq.DNASeqMPRADataset(\n",
    "#                 **dataset_kwargs,\n",
    "#                 split=None\n",
    "#                 )\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [\n",
    "    config.LabelColumnInfo(\n",
    "        label_column_name=\"dummy_label\", n_unique_values=1, is_regression_label=True\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare configurations required for running prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(dataset_kwargs, checkpoint_path, label_columns):\n",
    "    ckpt_dict = torch.load(checkpoint_path)\n",
    "    if \"model_config\" in ckpt_dict[\"hyper_parameters\"]:\n",
    "        fields = [i for i in ckpt_dict[\"hyper_parameters\"][\"model_config\"].fields if i.is_input]\n",
    "    else:\n",
    "        fields = config.main_config.default_fields()\n",
    "        tokenizer = load_tokenizer(Path(checkpoint_path).parent)\n",
    "        for f in fields:\n",
    "            f.update_vocab_size(tokenizer)\n",
    "        processed_name = \"processed\"\n",
    "\n",
    "\n",
    "    data_module = dnaseq.DNASeqMPRADataModule(  \n",
    "        tokenizer=load_tokenizer(Path(checkpoint_path).parent),\n",
    "        fields = fields,\n",
    "        max_length=512,\n",
    "        label_columns=label_columns,\n",
    "        collation_strategy=\"sequence_classification\",\n",
    "        batch_size = 32,\n",
    "        dataset_kwargs=dataset_kwargs,\n",
    "    )\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup(\"predict\")\n",
    "\n",
    "    label_dict = data_module.label_dict[label_columns[0].label_column_name] # pylint: disable=unsubscriptable-object\n",
    "\n",
    "    trainer_config = config.TrainerConfig(batch_size = 64, losses=None, pooling_method=\"first_token\")\n",
    "\n",
    "    task_config = config.PredictTaskConfig(\n",
    "        default_root_dir = environ[\"BMFM_TARGETS_TRAINING_DIR\"],    \n",
    "        output_predictions= True,\n",
    "        output_embeddings= True,\n",
    "        checkpoint=checkpoint_path,\n",
    "        )\n",
    "\n",
    "    tokenizer = load_tokenizer(Path(checkpoint_path).parent)\n",
    "\n",
    "    if \"model_config\" in ckpt_dict[\"hyper_parameters\"]:\n",
    "        model_config = ckpt_dict[\"hyper_parameters\"][\"model_config\"]\n",
    "        model_config.checkpoint = checkpoint_path\n",
    "        model_config.num_labels=len(label_dict)\n",
    "    else:\n",
    "        model_config = config.SCBertConfig(\n",
    "        checkpoint= checkpoint_path,\n",
    "        fields=fields,\n",
    "        num_labels=len(label_dict),\n",
    "        classifier_dropout=0,\n",
    "        hidden_dropout_prob=0,\n",
    "        attention_probs_dropout_prob=0,\n",
    "        )\n",
    "\n",
    "    main = config.main_config.SCBertMainConfig(data_module=data_module,task=task_config, model=model_config, trainer=trainer_config, fields=fields)\n",
    "    main.complete_config()\n",
    "\n",
    "    pl_module = task_utils.instantiate_module_from_checkpoint(\n",
    "            main.task, main.data_module, main.model, main.trainer\n",
    "        )\n",
    "    results = task_utils.predict(pl_module, data_module, task_config)\n",
    "    embeds = results['embeddings']\n",
    "\n",
    "    cos = []\n",
    "    dot = []\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    N = int(len(embeds)/2)\n",
    "\n",
    "    for a in range(N):\n",
    "        ref_out = embeds[a]\n",
    "        alt_out = embeds[a+N]\n",
    "        cos.append((ref_out * alt_out).sum()/(np.linalg.norm(ref_out)*np.linalg.norm(alt_out)))\n",
    "        dot.append((ref_out * alt_out).sum())\n",
    "        l1.append(np.absolute(ref_out - alt_out).sum())\n",
    "        l2.append(np.square(ref_out - alt_out).sum())\n",
    "\n",
    "    cagi_result = {'cos': cos, 'dot': dot, 'l1': l1, 'l2': l2}\n",
    "\n",
    "    return cagi_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TERT-GBM' 'IRF6' 'GP1BB' 'HBB' 'PKLR' 'SORT1' 'MSMB' 'MYCrs6983267'\n",
      " 'ZFAND3' 'HNF4A' 'TERT-HEK293T' 'HBG1' 'LDLR' 'IRF4' 'F9']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cagi_result = get_embeds(dataset_kwargs, checkpoint_path, label_columns)\n",
    "\n",
    "#cagi_result = h5py.File('../data/CAGI/'+'cagi_'+datalen+'_'+'dnabert2.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18442, 11)\n"
     ]
    }
   ],
   "source": [
    "cagi_df = pd.read_csv('/dccstor/bmfm-targets1/users/sanjoy/tools/LLM_eval/data/CAGI/4096/final_cagi_metadata.csv',\n",
    "                      index_col=0).reset_index()\n",
    "target = cagi_df['6'].values.tolist()\n",
    "exp_list = cagi_df['8'].values.tolist()\n",
    "cagi_df.head()\n",
    "print(cagi_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos\n",
      "LDLR\n",
      "0.0015450893499268405\n",
      "SORT1\n",
      "-0.19116159127891397\n",
      "F9\n",
      "0.00254269970811529\n",
      "PKLR\n",
      "0.0983730996423263\n",
      "dot\n",
      "LDLR\n",
      "0.1575734506587024\n",
      "SORT1\n",
      "-0.11913160366694353\n",
      "F9\n",
      "-0.20712877126917972\n",
      "PKLR\n",
      "-0.11220313065309032\n",
      "l1\n",
      "LDLR\n",
      "0.013581015718045254\n",
      "SORT1\n",
      "0.20186603003820217\n",
      "F9\n",
      "3.1194853220864634e-05\n",
      "PKLR\n",
      "-0.09416090487376942\n",
      "l2\n",
      "LDLR\n",
      "-0.0015822063988885022\n",
      "SORT1\n",
      "0.19111208313703\n",
      "F9\n",
      "-0.002713243433964822\n",
      "PKLR\n",
      "-0.09838357283979413\n"
     ]
    }
   ],
   "source": [
    "# This is for the ref_gen model....\n",
    "import scipy.stats as stats\n",
    "\n",
    "perf = []\n",
    "for key in cagi_result.keys():\n",
    "    print(key)\n",
    "    cagi_llr = cagi_result[key]\n",
    "    for exp in ['LDLR','SORT1','F9','PKLR']:\n",
    "        sub_df = cagi_df[cagi_df['8'] == exp]\n",
    "        exp_target = np.array(target)[sub_df.index.to_list()]\n",
    "        exp_pred = np.squeeze(cagi_llr)[sub_df.index.to_list()]\n",
    "        exp_target = np.absolute(exp_target)\n",
    "        exp_pred = np.absolute(exp_pred)\n",
    "        print(exp)\n",
    "        perf.append(stats.pearsonr(exp_pred,exp_target)[0])\n",
    "        print(stats.pearsonr(exp_pred,exp_target)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmfm-targets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
