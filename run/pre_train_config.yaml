defaults:
  - data_module: zheng68k_pre_training
  - tokenizer: all_genes
  - fields: genes_expressions_masked
  - trainer: default
  - task: train
  - model: scbert
  - _self_

seed:
  seed_value: 1234

model:
  checkpoint: null
  hidden_size: 16
  intermediate_size: 32
  num_hidden_layers: 2
  num_attention_heads: 2

data_module:
  max_length: 32
  data_dir: ${oc.env:BMFM_TARGETS_ZHENG68K_DATA}
  processed_name: processed_data
  batch_size: 20
  limit_genes: protein_coding #could be null, protein_coding, or tokenizer


trainer:
  learning_rate: 1.0e-5
  losses:
    - name: cross_entropy
      label_column_name: ${label_column_name}

task:
  accumulate_grad_batches: 5
  max_epochs: 100
  val_check_interval: 0.5

label_column_name: celltype
output_directory: /tmp
accelerator: gpu
val_check_interval: 0.5
max_epochs: 20
accumulate_grad_batches: 5