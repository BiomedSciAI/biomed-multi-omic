_target_: bmfm_targets.models.predictive.llama.LlamaForMultiTaskConfig
_partial_: true
num_hidden_layers: 12
num_attention_heads: 12
hidden_act: gelu
hidden_size: 384
initializer_range: 0.02
layer_norm_eps: 1.0e-12
pad_token_id: 0
flex_attention: true
use_cache: true
attention: flex
checkpoint: ${checkpoint_path}
